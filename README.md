# algo_trading
Python package library for data wrangling, backtesting and live trading strategies.

# data_wrangling package library.
The four stages of data wrangling, also known as data preprocessing or data preparation, typically involve the following steps:

1. Data Ingestion: This stage involves gathering the raw data from various sources, such as databases, files, APIs, or web scraping. It may include tasks like reading data files, connecting to databases, or retrieving data from external sources.

2. Data Cleaning: In this stage, the focus is on cleaning and transforming the data to address issues like missing values, inconsistent formats, duplicates, outliers, or errors. This step often involves tasks like handling missing data, removing duplicates, correcting data formats, and dealing with outliers or inconsistent values.

3. Data Transformation: Once the data is cleaned, the next stage involves transforming the data into a format suitable for analysis or modeling. This step may include tasks like feature engineering, scaling or normalizing variables, encoding categorical variables, or creating new derived variables.

4. Data Integration: The final stage is data integration, where multiple data sources or datasets are combined or merged to create a unified and consistent dataset for analysis. This step may involve joining or merging tables, aggregating data, or combining datasets based on common identifiers or keys.

These stages are iterative and often require going back and forth between them as new insights or issues are discovered during the analysis process. The goal of data wrangling is to prepare the data in a clean, structured, and consistent format to facilitate effective data analysis and modeling.
